{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Minecraft Block Prediction\n",
                "\n",
                "This is my first prototype of an AI which takes a photo from Minecraft and predicts where there are blocks and where there is air.\n",
                "\n",
                "This notebook\n",
                "  - Provides basic visuilization of Minecraft Block Arrays\n",
                "  - Preprocesses images for training\n",
                "  - Provides a naive solution to solve the problem (73% accuracy)\n",
                "  - Provides a CNN solution (83% accuracy)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "#TODO: Remove unnecessary imports \n",
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "from torchvision import datasets\n",
                "from torchvision.transforms import ToTensor\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import pandas as pd\n",
                "from torchvision.io import read_image\n",
                "\n",
                "from fastai.vision.all import *\n",
                "from fastdownload import *\n",
                "from fastbook import *\n",
                "import ipyvolume as ipv\n",
                "import numpy as np\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/home/ben/anaconda3/envs/ml2/lib/python3.8/site-packages/fastbook/__init__.py:19: UserWarning: Missing `graphviz` - please run `conda install fastbook`\n",
                        "  except ModuleNotFoundError: warn(\"Missing `graphviz` - please run `conda install fastbook`\")\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "train_path = Path('/home/ben/Desktop/Python/BlockDetection')\n",
                "test_path = Path('/home/ben/Desktop/Python/BlockDetectionValidation')\n",
                "train_files = train_path.ls()\n",
                "test_files = test_path.ls()\n",
                "train_files[0],  str(train_files[0].name)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(Path('/home/ben/Desktop/Python/BlockDetection/biome.minecraft.taiga_hills283.txt'),\n",
                            " 'biome.minecraft.taiga_hills283.txt')"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 3
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "len(train_files[train_files.map(lambda f:f.is_file())]), len(train_files)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(20180, 20181)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 4
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "biome_types = L()\n",
                "for file in train_files:\n",
                "    match = re.findall('.*biome.minecraft.([a-z_]*)\\d*.txt', str(file))\n",
                "    if (len(match) != 0):\n",
                "        biome_types.append(match[0])\n",
                "\n",
                "biome_types = biome_types.unique()\n",
                "biome_types"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(#41) ['taiga_hills','desert','plains','flower_forest','giant_tree_taiga','jungle','birch_forest','taiga_mountains','mountains','taiga'...]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 5
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "def is_air(block):\n",
                "    return block == 'air' or block == \"cave_air\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "def to_tensor(data_file_path):\n",
                "    df = pd.read_json(str(data_file_path))\n",
                "    list_len = len(df[0][0])\n",
                "    data = np.zeros((df.shape[0],df.shape[1], len(df[0][0])), dtype=np.float32)\n",
                "    for x in range(0, df.shape[0]):\n",
                "        for y in range(0, df.shape[1]):\n",
                "            for z in range(0, list_len):\n",
                "                is_block = not is_air(df[y][x][z]) \n",
                "                data[x,y,z] = 1. if is_block else 0.\n",
                "    return data"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "to_tensor(train_files[0]).shape"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(20, 7, 10)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 8
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "tensor = to_tensor(Path('/home/ben/Desktop/Python/BlockDetectionValidation/biome.minecraft.birch_forest106.txt'))\n",
                "\n",
                "#Number of air blocks\n",
                "1400 - tensor.sum()"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "760.0"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "def to_scatter(block_tensor):\n",
                "    total = 0\n",
                "    a,b,c = [0],[0],[0]\n",
                "    for x in range(0, block_tensor.shape[0]):\n",
                "        for y in range(0, block_tensor.shape[1]):\n",
                "            for z in range(0, 10):#TODO: Fix\n",
                "                if (block_tensor[x][y][z] == 1):\n",
                "                    a.append(x)\n",
                "                    b.append(y)\n",
                "                    c.append(z)\n",
                "                    total = total + 1\n",
                "\n",
                "    return np.asarray(a, dtype=np.float32), np.asarray(b, dtype=np.float32), np.asarray(c, dtype=np.float32)\n",
                "\n",
                "tensor = to_tensor(Path('/home/ben/Desktop/Python/BlockDetectionValidation/biome.minecraft.birch_forest102.txt'))\n",
                "x_out, y_out, z_out = to_scatter(tensor)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "def show_scatter(block_tensor):\n",
                "    ipv.figure()\n",
                "    x_out, y_out, z_out = to_scatter(block_tensor)\n",
                "    ipv.scatter(x_out, y_out, z_out)\n",
                "    ipv.show()\n",
                "def show_scatter_from_path(path):\n",
                "    show_scatter(to_tensor(path))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def to_image_path(file_path):\n",
                "    image_file = str(file_path).replace('BlockDetectionValidation', 'BlockDetectionValidation/screenshots')\n",
                "    if \"BlockDetectionValidation\" not in image_file:\n",
                "        image_file = image_file.replace('BlockDetection', 'BlockDetection/screenshots')\n",
                "    image_file = image_file.replace('.txt', '.png')\n",
                "    return image_file"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "example_path = '/home/ben/Desktop/Python/BlockDetectionValidation/biome.minecraft.birch_forest104.txt'\n",
                "show_scatter_from_path(Path(example_path))\n",
                "im = to_image_path(example_path)\n",
                "Image.open(im)"
            ],
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "VBox(children=(Figure(camera=PerspectiveCamera(fov=45.0, position=(0.0, 0.0, 2.0), projectionMatrix=(1.0, 0.0,…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "c90bfa2f080f4722aa1f07af3588d7b5"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=100x100 at 0x7FB7B5A19250>"
                        ],
                        "image/png": "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"
                    },
                    "metadata": {},
                    "execution_count": 13
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "def prepare_images(path):\n",
                "    data_files =path.ls()\n",
                "\n",
                "    data_files =data_files[data_files.map(lambda f:f.is_file())]\n",
                "    len(data_files)\n",
                "\n",
                "    for data_file in data_files:\n",
                "        #image_file = str(data_file).replace('BlockDetection', 'BlockDetection/screenshots')\n",
                "        image_file = to_image_path(str(data_file))\n",
                "        open_image = Image.open(image_file)\n",
                "        if (open_image.mode != 'RGB'):\n",
                "            print('Updating File to be RGB 100x100')\n",
                "            Image.open(image_file).convert('RGB').save(image_file)\n",
                "            Image.open(image_file).resize((100, 100)).save(image_file)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "prepare_images(Path('/home/ben/Desktop/Python/BlockDetectionValidation'))\n",
                "prepare_images(Path('/home/ben/Desktop/Python/BlockDetection'))"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "import multiprocessing"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "#def process(dataset, file):\n",
                "    #dataset.\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
                "#load_size = 1000\n",
                "\n",
                "class CustomDataset(Dataset):\n",
                "    def __init__(self, data_path):\n",
                "        self.data_path = data_path\n",
                "        self.files = data_path.ls()\n",
                "        self.files = self.files[self.files.map(lambda f:f.is_file())]\n",
                "        #self.files = self.files[0:load_size]\n",
                "        self.images = self.files.map(lambda f: self.__getimage__(f))\n",
                "        self.y = self.files.map(lambda f: to_tensor(f))\n",
                "    def __len__(self):\n",
                "        return len(self.files)\n",
                "    def __getitem__(self, idx):\n",
                "        return self.images[idx], self.y[idx]\n",
                "    def __getimage__(self, path):\n",
                "        #print(path)\n",
                "        image_file = to_image_path(path)\n",
                "        image = read_image(image_file)\n",
                "        image_float = image / 255\n",
                "        return image_float\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "from torch.utils.data import DataLoader\n",
                "\n",
                "BATCH_SIZE = 64\n",
                "\n",
                "train_data_folder = Path('/home/ben/Desktop/Python/BlockDetection')\n",
                "test_data_folder = Path('/home/ben/Desktop/Python/BlockDetectionValidation')\n",
                "train_dataloader = DataLoader(CustomDataset(train_data_folder), batch_size=BATCH_SIZE, shuffle=True)\n",
                "test_dataloader = DataLoader(CustomDataset(test_data_folder), batch_size=64, shuffle=True)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "for X, y in train_dataloader:\n",
                "    print(X.shape)\n",
                "    print(y.shape)\n",
                "    break"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([64, 3, 100, 100])\n",
                        "torch.Size([64, 20, 7, 10])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "# Get cpu or gpu device for training.\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(\"Using {} device\".format(device))\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Using cuda device\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "a = 0\n",
                "size = len(train_dataloader.dataset)\n",
                "for batch, (X, y) in enumerate(train_dataloader):\n",
                "    y = y.to(device)\n",
                "    y = y.reshape(-1,1400)\n",
                "    a = a + torch.sum(y, dim= 0)\n",
                "    #print(a.shape) \n",
                "\n",
                "print(a.shape)\n",
                "print(a)\n",
                "a = a / size\n",
                "a = a > 0.5\n",
                "a = a.type(torch.float)\n",
                "\n",
                "\n",
                "print(a.sum() / 1400)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([1400])\n",
                        "tensor([14413., 15040., 15808.,  ...,  2938.,  2954.,  2941.], device='cuda:0')\n",
                        "tensor(0.2857, device='cuda:0')\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "size = len(train_dataloader.dataset)\n",
                "total_dif = 0\n",
                "for batch, (X, y) in enumerate(train_dataloader):\n",
                "    y = y.to(device)\n",
                "    y = y.reshape(-1,1400)\n",
                "    total_dif += (y - a).abs().sum()\n",
                "    #print(a.shape) \n",
                "\n",
                "train_accuracy = 1 - ((total_dif) / (1400 * size))\n",
                "\n",
                "\n",
                "size = len(test_dataloader.dataset)\n",
                "total_dif = 0\n",
                "for batch, (X, y) in enumerate(test_dataloader):\n",
                "    y = y.to(device)\n",
                "    y = y.reshape(-1,1400)\n",
                "    total_dif += (y - a).abs().sum()\n",
                "    #print(a.shape) \n",
                "\n",
                "test_accuracy = 1 - ((total_dif) / (1400 * size))\n",
                "\n",
                "\n",
                "print('Train accuracy ' + str(train_accuracy))\n",
                "print('Test accuracy ' + str(test_accuracy))\n",
                "\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Train accuracy tensor(0.7439, device='cuda:0')\n",
                        "Test accuracy tensor(0.7316, device='cuda:0')\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "def train(dataloader, model, loss_fn, optimizer):\n",
                "    size = len(dataloader.dataset)\n",
                "    model.train()\n",
                "    for batch, (X, y) in enumerate(dataloader):\n",
                "        X, y = X.to(device), y.to(device)\n",
                "        pred = model(X)\n",
                "        y =y.reshape(-1,1400)\n",
                "        loss = loss_fn(pred, y)\n",
                "\n",
                "        # Backpropagation\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        if batch % 100 == 0:\n",
                "            loss, current = loss.item(), batch * len(X)\n",
                "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "def test(dataloader, model, loss_fn):\n",
                "    num_batches = len(dataloader)\n",
                "    model.eval()\n",
                "    total = 0\n",
                "    test_loss, correct = 0, 0\n",
                "    with torch.no_grad():\n",
                "        for X, y in dataloader:\n",
                "            X, y = X.to(device), y.to(device).reshape(-1, 1400)\n",
                "            pred = model(X)\n",
                "            test_loss += loss_fn(pred, y).item()            \n",
                "            pred = pred > 0.5\n",
                "            #print(pred.shape)\n",
                "            #print('Pred shape, y shape' + str(pred.shape) + str(y.shape))\n",
                "            accuracy = (pred.type(torch.float) - y).abs().sum() / (1400 * y.shape[0])      \n",
                "            #print('Y sum' + str(y.sum()))\n",
                "            #print(pred.type(torch.float).sum())\n",
                "            #print('Acc:' + str(accuracy))\n",
                "            correct += accuracy.item()\n",
                "            total = total + 1\n",
                "    test_loss /= num_batches\n",
                "    correct /= total\n",
                "    correct = 1 - correct\n",
                "    print(\"Accuracy: \" + str(correct))\n",
                "    print(f\"Test loss: {test_loss:>8f} \\n\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "\n",
                "# Define model\n",
                "class NeuralNetwork(nn.Module):\n",
                "    def __init__(self):\n",
                "        #https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/\n",
                "        super(NeuralNetwork, self).__init__()\n",
                "        self.layer1 = nn.Sequential(\n",
                "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
                "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
                "            #nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
                "            #nn.ReLU(),          \n",
                "            )\n",
                "        self.layer2 =nn.Sequential(\n",
                "            nn.Conv2d(64,64, kernel_size=5, stride=1, padding=2),\n",
                "            nn.ReLU(),\n",
                "            \n",
                "            nn.Conv2d(64,64, kernel_size=5, stride=1, padding=2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(),\n",
                "            nn.Conv2d(64,128, kernel_size=5, stride=1, padding=2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(),\n",
                "            nn.Conv2d(128,128, kernel_size=5, stride=1, padding=2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout()\n",
                "            #nn.MaxPool2d(kernel_size=5, stride=2),\n",
                "            #nn.Conv2d(256,256, kernel_size=5, stride=1, padding=2),\n",
                "            #nn.ReLU(),\n",
                "            #nn.MaxPool2d(kernel_size=5, stride=2),\n",
                "            )\n",
                "        self.dropout = nn.Dropout()\n",
                "        self.fc1 =nn.Linear(80000, 20*7*10)#80000  #36864 #7*7*64\n",
                "        #self.fc2 = nn.Linear(20*7*10, 20*7*10)\n",
                "        self.fc3 = nn.Sigmoid()\n",
                "        \n",
                "        #self.linear_relu_stack = nn.Sequential(\n",
                "        #    nn.Linear(3*100*100, 512),\n",
                "        #    nn.ReLU(),\n",
                "        #    nn.Linear(512, 512),\n",
                "        #    nn.ReLU(),\n",
                "        #    nn.Linear(512, 20*7*10)\n",
                "            #nn.Sigmoid\n",
                "        #)\n",
                "\n",
                "    def forward(self, x):\n",
                "        #print(x.shape)\n",
                "        out = self.layer1(x)\n",
                "        #print(out.shape)\n",
                "        out = self.layer2(out)\n",
                "        out = out.reshape(out.size(0), -1)#\n",
                "        out = self.dropout(out)\n",
                "        #print('Out size' + str(out.size()))\n",
                "        out = self.fc1(out)\n",
                "        #out = self.fc2(out)\n",
                "        out = self.fc3(out)\n",
                "        #logits = self.linear_relu_stack(x)\n",
                "        return out\n",
                "\n",
                "##model = NeuralNetwork().to(device)\n",
                "#print(model)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "model = NeuralNetwork().to(device)\n",
                "#loss_fn = nn.L1Loss()#nn.BCELoss() #nn.CrossEntropyLoss()\n",
                "loss_fn = nn.BCELoss()\n",
                "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "epochs = 1\n",
                "for t in range(epochs):\n",
                "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
                "    train(train_dataloader, model, loss_fn, optimizer)\n",
                "    #test(train_dataloader, model, loss_fn)\n",
                "    test(test_dataloader, model, loss_fn)    \n",
                "print(\"Done!\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 1\n",
                        "-------------------------------\n",
                        "loss: 0.693152  [    0/20180]\n",
                        "loss: 0.500489  [ 6400/20180]\n",
                        "loss: 0.478495  [12800/20180]\n",
                        "loss: 0.481296  [19200/20180]\n",
                        "Accuracy: 0.7644092777500981\n",
                        "Test loss: 0.489699 \n",
                        "\n",
                        "Done!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "def compute_dif(pred, actual):\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        pred = pred < 0.5\n",
                "        accuracy = (pred.type(torch.float) - actual).abs().sum() / (1400 * pred.size(0))\n",
                "        return accuracy\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "epochs = 30\n",
                "for t in range(epochs):\n",
                "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
                "    train(train_dataloader, model, loss_fn, optimizer)\n",
                "    #test(train_dataloader, model, loss_fn)\n",
                "    test(test_dataloader, model, loss_fn)  \n",
                "print(\"Done!\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 1\n",
                        "-------------------------------\n",
                        "loss: 0.432971  [    0/20180]\n",
                        "loss: 0.428522  [ 6400/20180]\n",
                        "loss: 0.458958  [12800/20180]\n",
                        "loss: 0.484970  [19200/20180]\n",
                        "Accuracy: 0.7698473710080852\n",
                        "Test loss: 0.488767 \n",
                        "\n",
                        "Epoch 2\n",
                        "-------------------------------\n",
                        "loss: 0.449430  [    0/20180]\n",
                        "loss: 0.450993  [ 6400/20180]\n",
                        "loss: 0.402605  [12800/20180]\n",
                        "loss: 0.374227  [19200/20180]\n",
                        "Accuracy: 0.7882811206838359\n",
                        "Test loss: 0.469235 \n",
                        "\n",
                        "Epoch 3\n",
                        "-------------------------------\n",
                        "loss: 0.424212  [    0/20180]\n",
                        "loss: 0.406855  [ 6400/20180]\n",
                        "loss: 0.373642  [12800/20180]\n",
                        "loss: 0.390643  [19200/20180]\n",
                        "Accuracy: 0.7916484786116559\n",
                        "Test loss: 0.449420 \n",
                        "\n",
                        "Epoch 4\n",
                        "-------------------------------\n",
                        "loss: 0.409904  [    0/20180]\n",
                        "loss: 0.357252  [ 6400/20180]\n",
                        "loss: 0.346799  [12800/20180]\n",
                        "loss: 0.366510  [19200/20180]\n",
                        "Accuracy: 0.7942759673232618\n",
                        "Test loss: 0.439383 \n",
                        "\n",
                        "Epoch 5\n",
                        "-------------------------------\n",
                        "loss: 0.376451  [    0/20180]\n",
                        "loss: 0.386219  [ 6400/20180]\n",
                        "loss: 0.359027  [12800/20180]\n",
                        "loss: 0.329389  [19200/20180]\n",
                        "Accuracy: 0.7930137048596921\n",
                        "Test loss: 0.440635 \n",
                        "\n",
                        "Epoch 6\n",
                        "-------------------------------\n",
                        "loss: 0.378729  [    0/20180]\n",
                        "loss: 0.418011  [ 6400/20180]\n",
                        "loss: 0.310787  [12800/20180]\n",
                        "loss: 0.378531  [19200/20180]\n",
                        "Accuracy: 0.7961210589046064\n",
                        "Test loss: 0.439417 \n",
                        "\n",
                        "Epoch 7\n",
                        "-------------------------------\n",
                        "loss: 0.428923  [    0/20180]\n",
                        "loss: 0.372149  [ 6400/20180]\n",
                        "loss: 0.340096  [12800/20180]\n",
                        "loss: 0.358075  [19200/20180]\n",
                        "Accuracy: 0.7946230529443077\n",
                        "Test loss: 0.441601 \n",
                        "\n",
                        "Epoch 8\n",
                        "-------------------------------\n",
                        "loss: 0.384622  [    0/20180]\n",
                        "loss: 0.356498  [ 6400/20180]\n",
                        "loss: 0.388391  [12800/20180]\n",
                        "loss: 0.324341  [19200/20180]\n",
                        "Accuracy: 0.806880296572395\n",
                        "Test loss: 0.415440 \n",
                        "\n",
                        "Epoch 9\n",
                        "-------------------------------\n",
                        "loss: 0.343145  [    0/20180]\n",
                        "loss: 0.361070  [ 6400/20180]\n",
                        "loss: 0.325623  [12800/20180]\n",
                        "loss: 0.352868  [19200/20180]\n",
                        "Accuracy: 0.8049202904753063\n",
                        "Test loss: 0.420999 \n",
                        "\n",
                        "Epoch 10\n",
                        "-------------------------------\n",
                        "loss: 0.363070  [    0/20180]\n",
                        "loss: 0.312898  [ 6400/20180]\n",
                        "loss: 0.358722  [12800/20180]\n",
                        "loss: 0.353662  [19200/20180]\n",
                        "Accuracy: 0.8077063217111256\n",
                        "Test loss: 0.412725 \n",
                        "\n",
                        "Epoch 11\n",
                        "-------------------------------\n",
                        "loss: 0.358686  [    0/20180]\n",
                        "loss: 0.334106  [ 6400/20180]\n",
                        "loss: 0.411431  [12800/20180]\n",
                        "loss: 0.339003  [19200/20180]\n",
                        "Accuracy: 0.8224989434947139\n",
                        "Test loss: 0.388161 \n",
                        "\n",
                        "Epoch 12\n",
                        "-------------------------------\n",
                        "loss: 0.333277  [    0/20180]\n",
                        "loss: 0.345636  [ 6400/20180]\n",
                        "loss: 0.325643  [12800/20180]\n",
                        "loss: 0.328490  [19200/20180]\n",
                        "Accuracy: 0.8205041334680889\n",
                        "Test loss: 0.389156 \n",
                        "\n",
                        "Epoch 13\n",
                        "-------------------------------\n",
                        "loss: 0.373367  [    0/20180]\n",
                        "loss: 0.353150  [ 6400/20180]\n",
                        "loss: 0.341843  [12800/20180]\n",
                        "loss: 0.348222  [19200/20180]\n",
                        "Accuracy: 0.8204208437515341\n",
                        "Test loss: 0.393116 \n",
                        "\n",
                        "Epoch 14\n",
                        "-------------------------------\n",
                        "loss: 0.328015  [    0/20180]\n",
                        "loss: 0.331847  [ 6400/20180]\n",
                        "loss: 0.305410  [12800/20180]\n",
                        "loss: 0.339349  [19200/20180]\n",
                        "Accuracy: 0.8193709435670272\n",
                        "Test loss: 0.392866 \n",
                        "\n",
                        "Epoch 15\n",
                        "-------------------------------\n",
                        "loss: 0.331596  [    0/20180]\n",
                        "loss: 0.291015  [ 6400/20180]\n",
                        "loss: 0.351618  [12800/20180]\n",
                        "loss: 0.283422  [19200/20180]\n",
                        "Accuracy: 0.8185312871051871\n",
                        "Test loss: 0.395232 \n",
                        "\n",
                        "Epoch 16\n",
                        "-------------------------------\n",
                        "loss: 0.334520  [    0/20180]\n",
                        "loss: 0.334860  [ 6400/20180]\n",
                        "loss: 0.331579  [12800/20180]\n",
                        "loss: 0.331671  [19200/20180]\n",
                        "Accuracy: 0.8167990394260572\n",
                        "Test loss: 0.397313 \n",
                        "\n",
                        "Epoch 17\n",
                        "-------------------------------\n",
                        "loss: 0.311781  [    0/20180]\n",
                        "loss: 0.386234  [ 6400/20180]\n",
                        "loss: 0.324245  [12800/20180]\n",
                        "loss: 0.293414  [19200/20180]\n",
                        "Accuracy: 0.8246987960908724\n",
                        "Test loss: 0.383916 \n",
                        "\n",
                        "Epoch 18\n",
                        "-------------------------------\n",
                        "loss: 0.333982  [    0/20180]\n",
                        "loss: 0.334125  [ 6400/20180]\n",
                        "loss: 0.365832  [12800/20180]\n",
                        "loss: 0.335470  [19200/20180]\n",
                        "Accuracy: 0.826022950851399\n",
                        "Test loss: 0.384714 \n",
                        "\n",
                        "Epoch 19\n",
                        "-------------------------------\n",
                        "loss: 0.314690  [    0/20180]\n",
                        "loss: 0.339606  [ 6400/20180]\n",
                        "loss: 0.369145  [12800/20180]\n",
                        "loss: 0.292767  [19200/20180]\n",
                        "Accuracy: 0.8216951949440915\n",
                        "Test loss: 0.388614 \n",
                        "\n",
                        "Epoch 20\n",
                        "-------------------------------\n",
                        "loss: 0.339106  [    0/20180]\n",
                        "loss: 0.316888  [ 6400/20180]\n",
                        "loss: 0.332603  [12800/20180]\n",
                        "loss: 0.352448  [19200/20180]\n",
                        "Accuracy: 0.8263361130071722\n",
                        "Test loss: 0.382178 \n",
                        "\n",
                        "Epoch 21\n",
                        "-------------------------------\n",
                        "loss: 0.311775  [    0/20180]\n",
                        "loss: 0.299061  [ 6400/20180]\n",
                        "loss: 0.291525  [12800/20180]\n",
                        "loss: 0.329886  [19200/20180]\n",
                        "Accuracy: 0.8247993303381879\n",
                        "Test loss: 0.385343 \n",
                        "\n",
                        "Epoch 22\n",
                        "-------------------------------\n",
                        "loss: 0.325931  [    0/20180]\n",
                        "loss: 0.305188  [ 6400/20180]\n",
                        "loss: 0.321261  [12800/20180]\n",
                        "loss: 0.334003  [19200/20180]\n",
                        "Accuracy: 0.8257385945838431\n",
                        "Test loss: 0.384814 \n",
                        "\n",
                        "Epoch 23\n",
                        "-------------------------------\n",
                        "loss: 0.300181  [    0/20180]\n",
                        "loss: 0.301908  [ 6400/20180]\n",
                        "loss: 0.317741  [12800/20180]\n",
                        "loss: 0.325007  [19200/20180]\n",
                        "Accuracy: 0.8174735793600911\n",
                        "Test loss: 0.395468 \n",
                        "\n",
                        "Epoch 24\n",
                        "-------------------------------\n",
                        "loss: 0.307646  [    0/20180]\n",
                        "loss: 0.289113  [ 6400/20180]\n",
                        "loss: 0.357412  [12800/20180]\n",
                        "loss: 0.347994  [19200/20180]\n",
                        "Accuracy: 0.8266838581665703\n",
                        "Test loss: 0.378769 \n",
                        "\n",
                        "Epoch 25\n",
                        "-------------------------------\n",
                        "loss: 0.307176  [    0/20180]\n",
                        "loss: 0.266249  [ 6400/20180]\n",
                        "loss: 0.309504  [12800/20180]\n",
                        "loss: 0.301662  [19200/20180]\n",
                        "Accuracy: 0.8273717184429583\n",
                        "Test loss: 0.380500 \n",
                        "\n",
                        "Epoch 26\n",
                        "-------------------------------\n",
                        "loss: 0.318881  [    0/20180]\n",
                        "loss: 0.301445  [ 6400/20180]\n",
                        "loss: 0.296329  [12800/20180]\n",
                        "loss: 0.322420  [19200/20180]\n",
                        "Accuracy: 0.8292063558879106\n",
                        "Test loss: 0.375704 \n",
                        "\n",
                        "Epoch 27\n",
                        "-------------------------------\n",
                        "loss: 0.360940  [    0/20180]\n",
                        "loss: 0.327355  [ 6400/20180]\n",
                        "loss: 0.289731  [12800/20180]\n",
                        "loss: 0.300410  [19200/20180]\n",
                        "Accuracy: 0.8190709715304167\n",
                        "Test loss: 0.398830 \n",
                        "\n",
                        "Epoch 28\n",
                        "-------------------------------\n",
                        "loss: 0.348900  [    0/20180]\n",
                        "loss: 0.305305  [ 6400/20180]\n",
                        "loss: 0.296230  [12800/20180]\n",
                        "loss: 0.337490  [19200/20180]\n",
                        "Accuracy: 0.8180784628443096\n",
                        "Test loss: 0.399311 \n",
                        "\n",
                        "Epoch 29\n",
                        "-------------------------------\n",
                        "loss: 0.340342  [    0/20180]\n",
                        "loss: 0.299321  [ 6400/20180]\n",
                        "loss: 0.322398  [12800/20180]\n",
                        "loss: 0.306089  [19200/20180]\n",
                        "Accuracy: 0.8248872471892316\n",
                        "Test loss: 0.389527 \n",
                        "\n",
                        "Epoch 30\n",
                        "-------------------------------\n",
                        "loss: 0.273911  [    0/20180]\n",
                        "loss: 0.304476  [ 6400/20180]\n",
                        "loss: 0.304260  [12800/20180]\n",
                        "loss: 0.308975  [19200/20180]\n",
                        "Accuracy: 0.8259217998255854\n",
                        "Test loss: 0.385998 \n",
                        "\n",
                        "Done!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "epochs = 30\n",
                "for t in range(epochs):\n",
                "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
                "    train(train_dataloader, model, loss_fn, optimizer)\n",
                "    #test(train_dataloader, model, loss_fn)\n",
                "    test(test_dataloader, model, loss_fn)    \n",
                "print(\"Done!\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 1\n",
                        "-------------------------------\n",
                        "loss: 0.307439  [    0/20180]\n",
                        "loss: 0.317669  [ 6400/20180]\n",
                        "loss: 0.351856  [12800/20180]\n",
                        "loss: 0.295455  [19200/20180]\n",
                        "Accuracy: 0.8291703598654788\n",
                        "Test loss: 0.378889 \n",
                        "\n",
                        "Epoch 2\n",
                        "-------------------------------\n",
                        "loss: 0.276453  [    0/20180]\n",
                        "loss: 0.350203  [ 6400/20180]\n",
                        "loss: 0.283634  [12800/20180]\n",
                        "loss: 0.272893  [19200/20180]\n",
                        "Accuracy: 0.8269932257092517\n",
                        "Test loss: 0.382221 \n",
                        "\n",
                        "Epoch 3\n",
                        "-------------------------------\n",
                        "loss: 0.272122  [    0/20180]\n",
                        "loss: 0.258076  [ 6400/20180]\n",
                        "loss: 0.304869  [12800/20180]\n",
                        "loss: 0.304071  [19200/20180]\n",
                        "Accuracy: 0.8258428366287895\n",
                        "Test loss: 0.386450 \n",
                        "\n",
                        "Epoch 4\n",
                        "-------------------------------\n",
                        "loss: 0.290387  [    0/20180]\n",
                        "loss: 0.311041  [ 6400/20180]\n",
                        "loss: 0.270576  [12800/20180]\n",
                        "loss: 0.293333  [19200/20180]\n",
                        "Accuracy: 0.8318854570388794\n",
                        "Test loss: 0.371883 \n",
                        "\n",
                        "Epoch 5\n",
                        "-------------------------------\n",
                        "loss: 0.327142  [    0/20180]\n",
                        "loss: 0.292779  [ 6400/20180]\n",
                        "loss: 0.309437  [12800/20180]\n",
                        "loss: 0.318608  [19200/20180]\n",
                        "Accuracy: 0.830027927523074\n",
                        "Test loss: 0.376334 \n",
                        "\n",
                        "Epoch 6\n",
                        "-------------------------------\n",
                        "loss: 0.259974  [    0/20180]\n",
                        "loss: 0.328136  [ 6400/20180]\n",
                        "loss: 0.243874  [12800/20180]\n",
                        "loss: 0.270295  [19200/20180]\n",
                        "Accuracy: 0.8294640669356221\n",
                        "Test loss: 0.380102 \n",
                        "\n",
                        "Epoch 7\n",
                        "-------------------------------\n",
                        "loss: 0.291169  [    0/20180]\n",
                        "loss: 0.233991  [ 6400/20180]\n",
                        "loss: 0.297736  [12800/20180]\n",
                        "loss: 0.335738  [19200/20180]\n",
                        "Accuracy: 0.8282547398753788\n",
                        "Test loss: 0.379537 \n",
                        "\n",
                        "Epoch 8\n",
                        "-------------------------------\n",
                        "loss: 0.277934  [    0/20180]\n",
                        "loss: 0.266975  [ 6400/20180]\n",
                        "loss: 0.274518  [12800/20180]\n",
                        "loss: 0.328477  [19200/20180]\n",
                        "Accuracy: 0.826158291619757\n",
                        "Test loss: 0.386709 \n",
                        "\n",
                        "Epoch 9\n",
                        "-------------------------------\n",
                        "loss: 0.296088  [    0/20180]\n",
                        "loss: 0.297877  [ 6400/20180]\n",
                        "loss: 0.253090  [12800/20180]\n",
                        "loss: 0.283294  [19200/20180]\n",
                        "Accuracy: 0.8305897116661072\n",
                        "Test loss: 0.376086 \n",
                        "\n",
                        "Epoch 10\n",
                        "-------------------------------\n",
                        "loss: 0.292305  [    0/20180]\n",
                        "loss: 0.291922  [ 6400/20180]\n",
                        "loss: 0.317625  [12800/20180]\n",
                        "loss: 0.300674  [19200/20180]\n",
                        "Accuracy: 0.8281253977962162\n",
                        "Test loss: 0.386106 \n",
                        "\n",
                        "Epoch 11\n",
                        "-------------------------------\n",
                        "loss: 0.312168  [    0/20180]\n",
                        "loss: 0.262418  [ 6400/20180]\n",
                        "loss: 0.268032  [12800/20180]\n",
                        "loss: 0.260180  [19200/20180]\n",
                        "Accuracy: 0.8281225315902544\n",
                        "Test loss: 0.381979 \n",
                        "\n",
                        "Epoch 12\n",
                        "-------------------------------\n",
                        "loss: 0.278884  [    0/20180]\n",
                        "loss: 0.271270  [ 6400/20180]\n",
                        "loss: 0.282441  [12800/20180]\n",
                        "loss: 0.266694  [19200/20180]\n",
                        "Accuracy: 0.8299147298802501\n",
                        "Test loss: 0.375858 \n",
                        "\n",
                        "Epoch 13\n",
                        "-------------------------------\n",
                        "loss: 0.261771  [    0/20180]\n",
                        "loss: 0.260292  [ 6400/20180]\n",
                        "loss: 0.256869  [12800/20180]\n",
                        "loss: 0.285332  [19200/20180]\n",
                        "Accuracy: 0.8336469531059265\n",
                        "Test loss: 0.368887 \n",
                        "\n",
                        "Epoch 14\n",
                        "-------------------------------\n",
                        "loss: 0.301560  [    0/20180]\n",
                        "loss: 0.285863  [ 6400/20180]\n",
                        "loss: 0.247793  [12800/20180]\n",
                        "loss: 0.267481  [19200/20180]\n",
                        "Accuracy: 0.8299280988133472\n",
                        "Test loss: 0.377014 \n",
                        "\n",
                        "Epoch 15\n",
                        "-------------------------------\n",
                        "loss: 0.277571  [    0/20180]\n",
                        "loss: 0.272842  [ 6400/20180]\n",
                        "loss: 0.277445  [12800/20180]\n",
                        "loss: 0.294960  [19200/20180]\n",
                        "Accuracy: 0.828772014249926\n",
                        "Test loss: 0.376290 \n",
                        "\n",
                        "Epoch 16\n",
                        "-------------------------------\n",
                        "loss: 0.281193  [    0/20180]\n",
                        "loss: 0.275522  [ 6400/20180]\n",
                        "loss: 0.239385  [12800/20180]\n",
                        "loss: 0.278243  [19200/20180]\n",
                        "Accuracy: 0.8287611629651941\n",
                        "Test loss: 0.380951 \n",
                        "\n",
                        "Epoch 17\n",
                        "-------------------------------\n",
                        "loss: 0.259410  [    0/20180]\n",
                        "loss: 0.246776  [ 6400/20180]\n",
                        "loss: 0.247891  [12800/20180]\n",
                        "loss: 0.315126  [19200/20180]\n",
                        "Accuracy: 0.8290093890998674\n",
                        "Test loss: 0.377342 \n",
                        "\n",
                        "Epoch 18\n",
                        "-------------------------------\n",
                        "loss: 0.279406  [    0/20180]\n",
                        "loss: 0.242786  [ 6400/20180]\n",
                        "loss: 0.283686  [12800/20180]\n",
                        "loss: 0.265621  [19200/20180]\n",
                        "Accuracy: 0.8323666038720504\n",
                        "Test loss: 0.377834 \n",
                        "\n",
                        "Epoch 19\n",
                        "-------------------------------\n",
                        "loss: 0.258774  [    0/20180]\n",
                        "loss: 0.259050  [ 6400/20180]\n",
                        "loss: 0.244112  [12800/20180]\n",
                        "loss: 0.242902  [19200/20180]\n",
                        "Accuracy: 0.8290774122528408\n",
                        "Test loss: 0.379539 \n",
                        "\n",
                        "Epoch 20\n",
                        "-------------------------------\n",
                        "loss: 0.228610  [    0/20180]\n",
                        "loss: 0.275755  [ 6400/20180]\n",
                        "loss: 0.245776  [12800/20180]\n",
                        "loss: 0.272789  [19200/20180]\n",
                        "Accuracy: 0.8285471686850423\n",
                        "Test loss: 0.380638 \n",
                        "\n",
                        "Epoch 21\n",
                        "-------------------------------\n",
                        "loss: 0.295151  [    0/20180]\n",
                        "loss: 0.230617  [ 6400/20180]\n",
                        "loss: 0.260618  [12800/20180]\n",
                        "loss: 0.275180  [19200/20180]\n",
                        "Accuracy: 0.8284649842459222\n",
                        "Test loss: 0.378671 \n",
                        "\n",
                        "Epoch 22\n",
                        "-------------------------------\n",
                        "loss: 0.291684  [    0/20180]\n",
                        "loss: 0.280345  [ 6400/20180]\n",
                        "loss: 0.256754  [12800/20180]\n",
                        "loss: 0.282209  [19200/20180]\n",
                        "Accuracy: 0.8305944748546766\n",
                        "Test loss: 0.376782 \n",
                        "\n",
                        "Epoch 23\n",
                        "-------------------------------\n",
                        "loss: 0.281434  [    0/20180]\n",
                        "loss: 0.309160  [ 6400/20180]\n",
                        "loss: 0.277075  [12800/20180]\n",
                        "loss: 0.254364  [19200/20180]\n",
                        "Accuracy: 0.8313197895236637\n",
                        "Test loss: 0.376881 \n",
                        "\n",
                        "Epoch 24\n",
                        "-------------------------------\n",
                        "loss: 0.266253  [    0/20180]\n",
                        "loss: 0.275492  [ 6400/20180]\n",
                        "loss: 0.307256  [12800/20180]\n",
                        "loss: 0.273820  [19200/20180]\n",
                        "Accuracy: 0.8305489045122395\n",
                        "Test loss: 0.380081 \n",
                        "\n",
                        "Epoch 25\n",
                        "-------------------------------\n",
                        "loss: 0.278985  [    0/20180]\n",
                        "loss: 0.272113  [ 6400/20180]\n",
                        "loss: 0.253551  [12800/20180]\n",
                        "loss: 0.264529  [19200/20180]\n",
                        "Accuracy: 0.8295719694832097\n",
                        "Test loss: 0.379484 \n",
                        "\n",
                        "Epoch 26\n",
                        "-------------------------------\n",
                        "loss: 0.259208  [    0/20180]\n",
                        "loss: 0.236440  [ 6400/20180]\n",
                        "loss: 0.256501  [12800/20180]\n",
                        "loss: 0.296363  [19200/20180]\n",
                        "Accuracy: 0.8276847043763036\n",
                        "Test loss: 0.383500 \n",
                        "\n",
                        "Epoch 27\n",
                        "-------------------------------\n",
                        "loss: 0.266819  [    0/20180]\n",
                        "loss: 0.262158  [ 6400/20180]\n",
                        "loss: 0.263044  [12800/20180]\n",
                        "loss: 0.209255  [19200/20180]\n",
                        "Accuracy: 0.8305994162093038\n",
                        "Test loss: 0.379124 \n",
                        "\n",
                        "Epoch 28\n",
                        "-------------------------------\n",
                        "loss: 0.207928  [    0/20180]\n",
                        "loss: 0.250326  [ 6400/20180]\n",
                        "loss: 0.241417  [12800/20180]\n",
                        "loss: 0.283073  [19200/20180]\n",
                        "Accuracy: 0.828192804818568\n",
                        "Test loss: 0.383215 \n",
                        "\n",
                        "Epoch 29\n",
                        "-------------------------------\n",
                        "loss: 0.266802  [    0/20180]\n",
                        "loss: 0.237211  [ 6400/20180]\n",
                        "loss: 0.248098  [12800/20180]\n",
                        "loss: 0.272584  [19200/20180]\n",
                        "Accuracy: 0.8282463148884152\n",
                        "Test loss: 0.386090 \n",
                        "\n",
                        "Epoch 30\n",
                        "-------------------------------\n",
                        "loss: 0.213165  [    0/20180]\n",
                        "loss: 0.258195  [ 6400/20180]\n",
                        "loss: 0.252708  [12800/20180]\n",
                        "loss: 0.262057  [19200/20180]\n",
                        "Accuracy: 0.8321966334529545\n",
                        "Test loss: 0.381342 \n",
                        "\n",
                        "Done!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# I don't know if anything after this works"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "test_data_folder = Path('/home/ben/Desktop/Python/BlockDetectionValidation')\n",
                "test_dataloader = DataLoader(CustomDataset(test_data_folder), batch_size=64, shuffle=True)\n",
                "\n",
                "epochs = 1\n",
                "for t in range(epochs):\n",
                "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
                "    #test(train_dataloader, model, loss_fn)\n",
                "    test(test_dataloader, model, loss_fn)    \n",
                "print(\"Done!\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 1\n",
                        "-------------------------------\n",
                        "Accuracy: 0.7751983786406724\n",
                        "Test loss: 1.466246 \n",
                        "\n",
                        "Done!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "source": [
                "def predict_image(path):\n",
                "    with torch.no_grad():\n",
                "\n",
                "        #image_file = to_image_path(data_file)\n",
                "        #image = read_image(image_file)\n",
                "        #image_float = image / 255\n",
                "\n",
                "        image = read_image(path) / 255\n",
                "        image_float = image.reshape(1,3,100,100).cuda()\n",
                "        pred = model(image_float)#.reshape(20, 7, 10)\n",
                "        pred = pred > 0.5\n",
                "        return pred.detach().cpu()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "def predict_image2(path):\n",
                "    with torch.no_grad():\n",
                "        image = read_image(path) / 255\n",
                "        image_float = image.reshape(1,3,100,100).cuda()\n",
                "        pred = model(image_float)#.reshape(20, 7, 10)\n",
                "        #pred = pred > 0.5\n",
                "        #return pred.detach().cpu().numpy()\n",
                "        return pred"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "source": [
                "validation_path = Path('/home/ben/Desktop/Python/BlockDetectionValidation')\n",
                "Path('/home/ben/Desktop/Python/BlockDetectionValidation/Predictions').mkdir(parents=true, exist_ok=true)\n",
                "validation_files = validation_path.ls()\n",
                "\n",
                "for file in validation_files:\n",
                "    if (file.is_file()):\n",
                "        image_file = to_image_path(file)\n",
                "        actual = to_tensor(file)\n",
                "        prediction = predict_image(image_file)\n",
                "        #print(prediction.shape)\n",
                "        difference = compute_dif(prediction, actual.reshape(1, -1))\n",
                "        prediction = prediction.reshape(20,7,10).numpy()\n",
                "\n",
                "        print(prediction.shape)\n",
                "        print(actual.shape)\n",
                "\n",
                "        print('Actual')\n",
                "        show_scatter(actual)\n",
                "        print('Prediction')\n",
                "        show_scatter(prediction)\n",
                "        print('Diff')\n",
                "        print(type(actual))\n",
                "        dif = actual - prediction\n",
                "        show_scatter(dif)\n",
                "        \n",
                "\n",
                "\n",
                "        print(difference)\n",
                "        print(file)\n",
                "        print(actual.sum())\n",
                "        break\n",
                "\n",
                "\n",
                "        #prediction_output_path = str(file).replace('BlockDetectionValidation', 'BlockDetectionValidation/Predictions')\n",
                "        #print(prediction_output_path)\n",
                "        #np.savetxt(prediction_output_path, prediction)\n",
                "        "
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(20, 7, 10)\n",
                        "(20, 7, 10)\n",
                        "Actual\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "VBox(children=(Figure(camera=PerspectiveCamera(fov=45.0, position=(0.0, 0.0, 2.0), projectionMatrix=(1.0, 0.0,…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "710d6a12c85b4aec955d51d9d85e65f4"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Prediction\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "VBox(children=(Figure(camera=PerspectiveCamera(fov=45.0, position=(0.0, 0.0, 2.0), projectionMatrix=(1.0, 0.0,…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "54559a5925cd4202af74e9ff669b9e9f"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Diff\n",
                        "<class 'numpy.ndarray'>\n"
                    ]
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": [
                            "VBox(children=(Figure(camera=PerspectiveCamera(fov=45.0, position=(0.0, 0.0, 2.0), projectionMatrix=(1.0, 0.0,…"
                        ],
                        "application/vnd.jupyter.widget-view+json": {
                            "version_major": 2,
                            "version_minor": 0,
                            "model_id": "1c4c6181a6c141329aad52f739116885"
                        }
                    },
                    "metadata": {}
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "tensor(0.9143)\n",
                        "/home/ben/Desktop/Python/BlockDetectionValidation/biome.minecraft.mountains331.txt\n",
                        "599.0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#with torch.no_grad():\n",
                "path = '/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'\n",
                "image = read_image(path) / 255\n",
                "image_float = image.reshape(1,3,100,100).cuda()\n",
                "\n",
                "pred = model(image_float).reshape(20, 7, 10)\n",
                "pred = pred > 0.5\n",
                "print(pred.shape)\n",
                "result = pred.detach().cpu().numpy()\n",
                "np.savetxt('output.csv', result)\n",
                "actual = to_tensor('/home/ben/Desktop/Python/BlockDetection/biome.minecraft.beach106.txt')\n",
                "actual = torch.tensor(actual)\n",
                "\n",
                "actual.shape, pred.shape\n",
                "\n",
                "print(pred.type())\n",
                "print(actual.type())\n",
                "actual_torch = torch.tensor(actual).cuda()\n",
                "pred_torch = torch.tensor(pred, dtype=torch.float)\n",
                "\n",
                "pred_torch.is_cuda\n",
                "\n",
                "dif_torch = actual_torch - pred_torch\n",
                "dif_torch.sum(), actual_torch.sum(), pred_torch.sum()\n"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "RuntimeError",
                    "evalue": "[Errno 2] No such file or directory: '/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-51-57e6aba4df5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/anaconda3/envs/ml2/lib/python3.8/site-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \"\"\"\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m~/anaconda3/envs/ml2/lib/python3.8/site-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: [Errno 2] No such file or directory: '/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "    #with torch.no_grad():\n",
                "    #path = '/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'\n",
                "    #image = read_image(path)\n",
                "    #image.resize(100, 100)\n",
                "    #image_float = image / 255\n",
                "    #print(image_float.shape)\n",
                "    #image_float = image_float.reshape(1,3,100,100).cuda()\n",
                "    #print(image_float.is_cuda)\n",
                "    #pred = model(image_float)#.reshape(20, 7, 10)\n",
                "    #print(str(pred))\n",
                "    #pred = pred > 0.5\n",
                "    #print(pred.shape)\n",
                "    #result = pred.detach().cpu().numpy()\n",
                "    #np.savetxt('output.csv', result)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([3, 100, 100])\n",
                        "True\n",
                        "torch.Size([1, 1400])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "actual = to_tensor('/home/ben/Desktop/Python/BlockDetection/biome.minecraft.beach106.txt')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "type(result), result"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(numpy.ndarray,\n",
                            " array([[ 0.7590272 ,  1.1494421 ,  1.1465473 , ...,  0.03555972,  0.13010326, -0.38936177]], dtype=float32))"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 70
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def save(obj, name):\n",
                "    "
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#with torch.no_grad():\n",
                "path = '/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'\n",
                "image = read_image(path) / 255\n",
                "image_float = image.reshape(1,3,100,100).cuda()\n",
                "\n",
                "pred = model(image_float).reshape(20, 7, 10)\n",
                "pred = pred > 0.5\n",
                "print(pred.shape)\n",
                "result = pred.detach().cpu().numpy()\n",
                "np.savetxt('output.csv', result)\n",
                "actual = to_tensor('/home/ben/Desktop/Python/BlockDetection/biome.minecraft.beach106.txt')\n",
                "actual = torch.tensor(actual)\n",
                "\n",
                "actual.shape, pred.shape\n",
                "\n",
                "print(pred.type())\n",
                "print(actual.type())\n",
                "actual_torch = torch.tensor(actual).cuda()\n",
                "pred_torch = torch.tensor(pred, dtype=torch.float)\n",
                "\n",
                "pred_torch.is_cuda\n",
                "\n",
                "dif_torch = actual_torch - pred_torch\n",
                "dif_torch.sum(), actual_torch.sum(), pred_torch.sum()\n"
            ],
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'read_image' is not defined",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-1-57e6aba4df5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#with torch.no_grad():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/ben/Desktop/Python/BlockDetection/screenshots/biome.minecraft.beach106.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimage_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'read_image' is not defined"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.8",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.8 64-bit ('ml2': conda)"
        },
        "interpreter": {
            "hash": "06482788fdae41cf3dc33faed5057546bb1dde851f20e69938e6ab8da0d8065e"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}